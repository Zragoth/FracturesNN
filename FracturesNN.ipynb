{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc357e1-163c-49a6-9f2a-9547e0e08aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# from numba import cuda\n",
    "# import torch\n",
    "##########\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "###########\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout,BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import iterstrat\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from PIL import Image  # Add this line\n",
    "\n",
    "import scipy.stats as stats\n",
    "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae76162-f232-460f-ad32-1b58f51df9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "def load_images_and_labels(folder_path, label_file, target_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            image_name, label = line.strip().split(',')\n",
    "            img = image.load_img(os.path.join(folder_path, image_name), target_size=target_size, color_mode='grayscale')\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array /= 255.0  # Normalize pixel values\n",
    "            images.append(img_array)\n",
    "            labels.append(int(label))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "###########\n",
    "# Plot and save loss graph\n",
    "def plot_loss(history,save_path=None):\n",
    "    file_name = \"_lossPlot.png\"\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path+file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Plot and save accuracy graph\n",
    "def plot_acc(history, save_path=None):\n",
    "    file_name = \"_accPlot.png\"\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path+file_name)\n",
    "    plt.show()\n",
    "        \n",
    "# Plot confusion matrix as heatmap\n",
    "def plot_confMatrix(conf_matrix,save_path=None):\n",
    "    file_name = \"_cnfMat.png\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, square=True, \n",
    "                xticklabels=['Class A', 'Class B', 'Class C'], \n",
    "                yticklabels=['Class A', 'Class B', 'Class C'])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path+file_name)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics(model_name, history, y_true, y_pred,y_test,x_test):\n",
    "    confidence_level = 0.95  # You can adjust this as needed\n",
    "    z_score = 1.96  # Z-score for 95% confidence interval\n",
    "    n_samples = len(y_true)\n",
    "    loss_test = history.history['val_loss'][-1]\n",
    "    acc_test = history.history['val_accuracy'][-1]\n",
    "    loss_train = history.history['loss'][-1]\n",
    "    acc_train = history.history['accuracy'][-1]\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    accuracy_ci = (accuracy_score(y_true, y_pred) - z_score * np.sqrt(accuracy_score(y_true, y_pred) * (1 - accuracy_score(y_true, y_pred)) / n_samples),\n",
    "               accuracy_score(y_true, y_pred) + z_score * np.sqrt(accuracy_score(y_true, y_pred) * (1 - accuracy_score(y_true, y_pred)) / n_samples))\n",
    "    descr = None\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Loss_test': loss_test,\n",
    "        'Acc_test': acc_test,\n",
    "        'Loss_train': loss_train,\n",
    "        'Acc_train': acc_train,\n",
    "        'F1 Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'accuracy_ci':accuracy_ci,\n",
    "        'descr': descr,\n",
    "    }\n",
    "\n",
    "\n",
    "# Save metrics table as image\n",
    "def metrics_table_pic(metrics_df, save_path=None):\n",
    "    file_name = \"_metrcs.png\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.table(cellText=metrics_df.values, colLabels=metrics_df.columns, loc='center', cellLoc='center')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path+file_name)\n",
    "    plt.show()\n",
    "\n",
    "# write metrics into a csv file\n",
    "def write_metrics_to_csv(metrics_dict):\n",
    "    csv_file = 'C:/Users/SIMIC/Downloads/Bilder_split/output/existing_metrics_table.csv'\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='index').T\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    if os.path.exists(csv_file):\n",
    "        try:\n",
    "            # Try reading the existing data\n",
    "            existing_data = pd.read_csv(csv_file)\n",
    "            if existing_data.empty:\n",
    "                # If the existing data is empty, write the metrics_df as the first entries\n",
    "                metrics_df.to_csv(csv_file, index=False)\n",
    "            else:\n",
    "                # If the existing data is not empty, append metrics_df to it\n",
    "                existing_data = pd.concat([existing_data, metrics_df], ignore_index=True)\n",
    "                existing_data.to_csv(csv_file, index=False)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            # Handle the case where the file exists but is empty\n",
    "            metrics_df.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If the file doesn't exist, write the metrics_df as the first entries\n",
    "        metrics_df.to_csv(csv_file, index=False)\n",
    "###########\n",
    "\n",
    "# image and label pats\n",
    "folder_path = r'C:/Users/SIMIC/Downloads/Bilder_split/ap2/'\n",
    "label_file = os.path.join(folder_path, 'labelsPrio1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f61463-0ba0-483a-ab9f-51e481ddb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main model\n",
    "\n",
    "#set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# define variables and hyperparameters\n",
    "descr = \"32/32/64/64/64-dense/EarlyStop\"\n",
    "loss= \"categorical_crossentropy\"\n",
    "optimizer= \"adam\"\n",
    "epochs= 50\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "target_size = (image_height, image_width)\n",
    "batch_size = 16\n",
    "l2_reg=0.01\n",
    "\n",
    "# prepare data set\n",
    "images, labels = load_images_and_labels(folder_path, label_file, target_size)\n",
    "labels = to_categorical(labels - 1) # onehot encoding\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(images) * split_ratio)\n",
    "x_train, y_train = images[:split_idx], labels[:split_idx]\n",
    "x_test, y_test = images[split_idx:], labels[split_idx:]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(image_height, image_width, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "    Dropout(0.75),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# train \n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# pred\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate classification metrics\n",
    "loss_test = history.history['val_loss'][-1]\n",
    "acc_test = history.history['val_accuracy'][-1]\n",
    "loss_train = history.history['loss'][-1]\n",
    "acc_train = history.history['accuracy'][-1]\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "# Accuarcy CI\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # Z-score for 95% confidence interval\n",
    "n_samples = len(y_true)\n",
    "\n",
    "accuracy_ci = (accuracy - z_score * np.sqrt(accuracy * (1 - accuracy) / n_samples),\n",
    "               accuracy + z_score * np.sqrt(accuracy * (1 - accuracy) / n_samples))\n",
    "\n",
    "# Print metrics\n",
    "metrics_dict = {\n",
    "    'Metric': ['Loss_test', 'Acc_test', 'Loss_train', 'Acc_train', 'F1 Score', 'Precision', 'Recall',\n",
    "               'acc_ci', 'descr'],\n",
    "    'Value': [loss_test, acc_test, loss_train, acc_train, f1, precision, recall,\n",
    "              accuracy_ci, descr]\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "plot_loss(history)\n",
    "plot_acc(history)\n",
    "plot_confMatrix(conf_matrix)\n",
    "print(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d1ff7-f9d3-45d1-9f90-fde3dcf5783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Model with Data augmentation\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# define variables and hyperparameters\n",
    "descr = \"32/32/64/64/64-dense/EarlyStop/dataAug\"\n",
    "loss= \"categorical_crossentropy\"\n",
    "optimizer= \"adam\"\n",
    "epochs= 50\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "target_size = (image_height, image_width)\n",
    "batch_size = 32\n",
    "l2_reg=0.01\n",
    "\n",
    "# prepare data set\n",
    "images, labels = load_images_and_labels(folder_path, label_file, target_size)\n",
    "labels = to_categorical(labels - 1) # onehot encoding\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(images) * split_ratio)\n",
    "x_train, y_train = images[:split_idx], labels[:split_idx]\n",
    "x_test, y_test = images[split_idx:], labels[split_idx:]\n",
    "\n",
    "model2 = Sequential([\n",
    "    Input(shape=(image_height, image_width, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "    Dropout(0.75),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Image Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='constant'\n",
    ")\n",
    "\n",
    "# Set up EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model2.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=int(len(x_train) / batch_size),\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "y_pred = np.argmax(model2.predict(x_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate classification metrics\n",
    "loss_test = history.history['val_loss'][-1]\n",
    "acc_test = history.history['val_accuracy'][-1]\n",
    "loss_train = history.history['loss'][-1]\n",
    "acc_train = history.history['accuracy'][-1]\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "# Accuarcy CI\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # Z-score for 95% confidence interval\n",
    "n_samples = len(y_true)\n",
    "\n",
    "accuracy_ci = (accuracy - z_score * np.sqrt(accuracy * (1 - accuracy) / n_samples),\n",
    "               accuracy + z_score * np.sqrt(accuracy * (1 - accuracy) / n_samples))\n",
    "\n",
    "# Print metrics\n",
    "metrics_dict = {\n",
    "    'Metric': ['Loss_test', 'Acc_test', 'Loss_train', 'Acc_train', 'F1 Score', 'Precision', 'Recall',\n",
    "               'acc_ci', 'descr'],\n",
    "    'Value': [loss_test, acc_test, loss_train, acc_train, f1, precision, recall,\n",
    "              accuracy_ci, descr]\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "plot_loss(history)\n",
    "plot_acc(history)\n",
    "plot_confMatrix(conf_matrix)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cbb09-27c3-4992-aee5-41b419699ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VGG16 and ResNet 50\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Define paths to your train and validation data directories\n",
    "train_data_dir = 'C:/Users/SIMIC/Downloads/Bilder_split/vgg16/training'\n",
    "val_data_dir = 'C:/Users/SIMIC/Downloads/Bilder_split/vgg16/validation'\n",
    "\n",
    "# Define batch size and epochs\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Custom data generator with preprocessing function\n",
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    def __init__(self, grayscale_to_rgb=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.grayscale_to_rgb = grayscale_to_rgb\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        if self.grayscale_to_rgb:\n",
    "            img = np.asarray(image.img_to_array(img.convert(\"RGB\")))\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x, batch_y = super().__getitem__(idx)\n",
    "        if self.grayscale_to_rgb:\n",
    "            batch_x = np.array([self.preprocess_img(img) for img in batch_x])\n",
    "        return batch_x, batch_y\n",
    "\n",
    "# Create train and validation generators with custom preprocessing function\n",
    "train_generator = CustomImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    grayscale_to_rgb=True  # Apply grayscale to RGB conversion\n",
    ").flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'  # Set to RGB\n",
    ")\n",
    "\n",
    "val_generator = CustomImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    grayscale_to_rgb=True  # Apply grayscale to RGB conversion\n",
    ").flow_from_directory(\n",
    "    val_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'  # Set to RGB\n",
    ")\n",
    "\n",
    "# Create and compile VGG16 model\n",
    "base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model_vgg16 = Sequential([\n",
    "    base_model_vgg16,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Update the number of neurons to 3\n",
    "])\n",
    "model_vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_vgg16 = model_vgg16.fit(train_generator, epochs=epochs, validation_data=val_generator)\n",
    "\n",
    "# Create and compile ResNet50 model\n",
    "base_model_resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model_resnet50 = Sequential([\n",
    "    base_model_resnet50,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  # Update the number of neurons to 3\n",
    "])\n",
    "model_resnet50.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_resnet50 = model_resnet50.fit(train_generator, epochs=epochs, validation_data=val_generator)\n",
    "\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(history_vgg16.history['accuracy'], label='VGG16 Training Accuracy')\n",
    "plt.plot(history_resnet50.history['accuracy'], label='ResNet50 Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ea3d7-6172-4bdc-8096-fbb580ee391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Path to the folder containing images\n",
    "image_folder = 'C:/Users/SIMIC/Downloads/Bilder_split/ap2/'\n",
    "\n",
    "# Path to the text file containing labels\n",
    "txt_file = 'C:/Users/SIMIC/Downloads/Bilder_split/ap2/labelsPrio1.txt'\n",
    "\n",
    "# Lists to store image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "def extract_features(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Read the text file line by line\n",
    "with open(txt_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\",\")  # Split each line based on comma separator\n",
    "        image_name = parts[0].strip()  # Extract image name\n",
    "        label = int(parts[1].strip())  # Extract label\n",
    "        image_path = os.path.join(image_folder, image_name)  # Construct full image path\n",
    "        image_paths.append(image_path)\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "X_train_features = np.array([extract_features(image_path) for image_path in X_train_paths])\n",
    "X_test_features = np.array([extract_features(image_path) for image_path in X_test_paths])\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_features.reshape(X_train_features.shape[0], -1), y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(X_test_features.reshape(X_test_features.shape[0], -1))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "n_iterations = 1000\n",
    "n_size = len(y_test)\n",
    "acc_results = np.zeros(n_iterations)\n",
    "for i in range(n_iterations):\n",
    "    # Bootstrap resample\n",
    "    indices = np.random.choice(len(y_test), size=n_size)\n",
    "    sample_X, sample_y = X_test_features[indices], y_test[indices]\n",
    "    # Predict using the trained model\n",
    "    yhat = rf_classifier.predict(sample_X.reshape(sample_X.shape[0], -1))\n",
    "    acc = accuracy_score(sample_y, yhat)\n",
    "    acc_results[i] = acc\n",
    "\n",
    "# Confidence intervals\n",
    "acc_ci = stats.t.interval(0.95, len(acc_results) - 1, loc=np.mean(acc_results), scale=stats.sem(acc_results))\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Accuracy Confidence Interval:\", acc_ci)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
